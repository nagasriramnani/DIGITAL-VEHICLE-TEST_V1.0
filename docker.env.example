# Virtual Testing Assistant - Docker Environment Configuration
# Copy this file to .env for docker-compose:
#   cp docker.env.example .env
# Then edit .env with your production passwords

# ============================================================================
# PostgreSQL Configuration
# ============================================================================
POSTGRES_DB=vta
POSTGRES_USER=vta_user
POSTGRES_PASSWORD=change_me_in_production
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# ============================================================================
# Neo4j Configuration
# ============================================================================
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=change_me_in_production
NEO4J_BOLT_PORT=7687
NEO4J_HTTP_PORT=7474

# ============================================================================
# Redis Configuration
# ============================================================================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=change_me_in_production

# ============================================================================
# API Configuration
# ============================================================================
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
LOG_LEVEL=info

# ============================================================================
# Dashboard Configuration
# ============================================================================
DASHBOARD_PORT=8501
API_BASE_URL=http://api:8000

# ============================================================================
# LLM Configuration
# ============================================================================
# Set to false to use real HuggingFace models (requires LangChain + transformers)
USE_MOCK_LLM=true

# HuggingFace model ID (used when USE_MOCK_LLM=false)
# Examples:
#   - mock-llm (for testing)
#   - gpt2 (small, fast, good for testing)
#   - mistralai/Mistral-7B-v0.1 (7B parameters, good balance)
#   - meta-llama/Llama-2-7b-chat-hf (7B parameters, chat-optimized)
#   - TinyLlama/TinyLlama-1.1B-Chat-v1.0 (1.1B parameters, very fast)
HF_LLM_MODEL_ID=mock-llm

# Device for model inference (auto/cuda/cpu)
# 'auto' will use GPU if available, otherwise CPU
HF_DEVICE=auto

# Enable 8-bit quantization (reduces memory, requires bitsandbytes)
HF_LOAD_8BIT=false

# Generation parameters
HF_MAX_NEW_TOKENS=512
HF_TEMPERATURE=0.2

# ============================================================================
# Business Configuration
# ============================================================================
HOURLY_RATE_GBP=75.0
PHYSICAL_TEST_MULTIPLIER=1.0
SIMULATION_COST_FACTOR=0.05

# ============================================================================
# Security (CHANGE IN PRODUCTION!)
# ============================================================================
SECRET_KEY=change_this_to_a_random_secret_key_in_production
CORS_ORIGINS=http://localhost:8501,http://localhost:3000

# ============================================================================
# Feature Flags
# ============================================================================
ENABLE_TELEMETRY=false
ENABLE_CACHING=true
ENABLE_DEBUG=false

